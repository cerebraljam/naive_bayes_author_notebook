{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Identifying the probable author of a message from it's previous texts\n",
    "\n",
    "Spam filters basically keep statistics of which words were seen in messages that were classified as ham or spam. For this notebook, I was curious: at first, could I compile statistics on the usage of words by few peoples, then with these statistics in hand, identify the probable author of a snipet of text?\n",
    "\n",
    "The success of this exercise highly depends on the quality of the text used as sources. However, it's an interesting toy project. \n",
    "\n",
    "I know that the subject is polarising, but I went with the last two US Presidents speeches for content.\n",
    "\n",
    "Sources:\n",
    "* Obama: few first speeches from http://obamaspeeches.com/ \n",
    "* Trump: https://www.tampabay.com/florida-politics/buzz/2018/08/01/heres-a-full-transcript-of-president-trumps-speech-from-his-tampa-rally/ and https://www.politico.com/story/2018/09/25/trump-un-speech-2018-full-text-transcript-840043\n",
    "\n",
    "And for the snipet of text, I used twitter:\n",
    "* tweet 1: https://twitter.com/BarackObama/status/1044690296917962754\n",
    "* tweet 2: https://twitter.com/realDonaldTrump/status/1045444544068812800\n",
    "* tweet 3: https://twitter.com/realDonaldTrump/status/1045003711104331776\n",
    "* tweet 4: https://twitter.com/BarackObama/status/1039512025406349312\n",
    "* tweet 5: https://twitter.com/BarackObama/status/1034892109868945409\n",
    "* tweet 6: https://twitter.com/realDonaldTrump/status/1043966388182953984\n",
    "\n",
    "**Spoiler**: Using these 6 examples, we have a 5/6 success rate, which isn't perfect, but still, it's a good start. The first tweet comes out negative (<12%) for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# I am reusing the same update_probability function used in my previous naive bayes notebooks\n",
    "from naive_bayes import update_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the source texts\n",
    "\n",
    "This function loads the files containing the text from each authors. The shape of the file doesn't matter too much since we will flat it out in a dataframe that contains \"author\" and \"word\" at first. Later on, we add the statistics to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(files):\n",
    "    df = pd.DataFrame(columns=['author', 'word'])\n",
    "    \n",
    "    for author, file in files.items():\n",
    "        data = pd.read_csv(file, names=['text'], delimiter='\\n')\n",
    "        data['author'] = author\n",
    "        data['text'] = data['text'].str.lower()\n",
    "        data['text'] = data['text'].str.replace('[^A-Za-z0-9\\@\\#\\']', ' ')\n",
    "        data['text'] = data['text'].str.replace('\\s+', ' ')\n",
    "        data['text'] = data['text'].str.strip()\n",
    "        data['text'] = data['text'].str.split()\n",
    "        for ll in data['text']:\n",
    "            for ww in ll:\n",
    "                df = df.append({'author': author, 'word': ww}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning the lines from unuseful characters\n",
    "\n",
    "This clean_line function looks a lot like the cleaning part of the load_data() function, however, because we are playing with pure text here instead of dataframes, I separated the part for the loading and cleaning. \n",
    "\n",
    "TODO: see if I can reuse something like this function for the load_data() function, instead of duplicating commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "\n",
    "    line = line.lower()\n",
    "    line = re.sub(r'[^\\w\\s\\'\\@\\#]',' ',line)\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    line = line.strip()\n",
    "    line = line.split(\" \")\n",
    "    return line\n",
    "\n",
    "def clean_lines(lines):\n",
    "    newlines = []\n",
    "    for l in lines:\n",
    "        newlines.append(clean_line(l))\n",
    "    return newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data\n",
    "\n",
    "This is where we load the training data and we assign the content to its owner. We can easily add more authors if desired, as opposed to traditional spam filters that only discriminate between good and bad text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28647</td>\n",
       "      <td>28647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>3374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>obama</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17980</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author   word\n",
       "count   28647  28647\n",
       "unique      2   3374\n",
       "top     obama    the\n",
       "freq    17980   1280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = {\n",
    "    'obama': 'training_obama.txt', \n",
    "    'trump': 'training_trump.txt'\n",
    "}\n",
    "\n",
    "\n",
    "df = load_data(files)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the statistics \n",
    "\n",
    "This function calculates the following:\n",
    "* How many time an author used a word, devided by everyone who used that word (true positives)\n",
    "* How many time others used that same word, devided by everyone who used that word (false positives)\n",
    "* True negatives = 1 - true positives\n",
    "* False negatives = 1 - false positives\n",
    "\n",
    "In addition, we keep the count of each word per author and for the others, which is useful for filtering later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_statistics(df):\n",
    "    metrics = pd.DataFrame(columns=['author', 'word', 'c_author','c_others', 'tp', 'tn', 'fp', 'fn'])\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if metrics.loc[(metrics['author'] == row['author'])& (metrics['word'] == row['word'])].empty:\n",
    "            count_word_for_author = len(df.loc[(df['word'] == row['word']) & (df['author'] == row['author'])].index)+1\n",
    "            count_word_for_others = len(df.loc[(df['word'] == row['word']) & (df['author'] != row['author'])].index)+1\n",
    "\n",
    "            metrics = metrics.append({\n",
    "                'author': row['author'],\n",
    "                'word': row['word'],\n",
    "                'c_author': count_word_for_author,\n",
    "                'c_others': count_word_for_others,\n",
    "                'tp': count_word_for_author/(count_word_for_author + count_word_for_others),\n",
    "                'tn': 1 - (count_word_for_author/(count_word_for_author + count_word_for_others)),\n",
    "                'fp': count_word_for_others/(count_word_for_author + count_word_for_others),\n",
    "                'fn': 1 - (count_word_for_others/(count_word_for_author + count_word_for_others))\n",
    "            }, ignore_index=True)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     author           word c_author c_others        tp        tn        fp  \\\n",
      "9     obama            the      828      454  0.645866  0.354134  0.354134   \n",
      "35    obama            and      756      391  0.659111  0.340889  0.340889   \n",
      "30    obama             to      536      311  0.632822  0.367178  0.367178   \n",
      "20    obama             of      471      230  0.671897  0.328103  0.328103   \n",
      "2353  trump            the      454      828  0.354134  0.645866  0.645866   \n",
      "101   obama           that      410      118  0.776515  0.223485  0.223485   \n",
      "2368  trump            and      391      756  0.340889  0.659111  0.659111   \n",
      "108   obama              a      352      163  0.683495  0.316505  0.316505   \n",
      "87    obama             we      316      216  0.593985  0.406015  0.406015   \n",
      "2344  trump             to      311      536  0.367178  0.632822  0.632822   \n",
      "83    obama             in      272      176  0.607143  0.392857  0.392857   \n",
      "23    obama            our      253      138  0.647059  0.352941  0.352941   \n",
      "3     obama              i      238      160  0.597990  0.402010  0.402010   \n",
      "2360  trump             of      230      471  0.328103  0.671897  0.671897   \n",
      "14    obama            for      225      114  0.663717  0.336283  0.336283   \n",
      "2351  trump             we      216      316  0.406015  0.593985  0.593985   \n",
      "64    obama             is      190      104  0.646259  0.353741  0.353741   \n",
      "41    obama           this      187       71  0.724806  0.275194  0.275194   \n",
      "2346  trump             in      176      272  0.392857  0.607143  0.607143   \n",
      "16    obama            you      168      154  0.521739  0.478261  0.478261   \n",
      "2356  trump              a      163      352  0.316505  0.683495  0.683495   \n",
      "2341  trump              i      160      238  0.402010  0.597990  0.597990   \n",
      "2386  trump            you      154      168  0.478261  0.521739  0.521739   \n",
      "259   obama            who      151       21  0.877907  0.122093  0.122093   \n",
      "96    obama             it      139      118  0.540856  0.459144  0.459144   \n",
      "2366  trump            our      138      253  0.352941  0.647059  0.647059   \n",
      "102   obama            are      136       79  0.632558  0.367442  0.367442   \n",
      "86    obama            but      136       59  0.697436  0.302564  0.302564   \n",
      "17    obama           have      126       99  0.560000  0.440000  0.440000   \n",
      "183   obama           will      120       54  0.689655  0.310345  0.310345   \n",
      "...     ...            ...      ...      ...       ...       ...       ...   \n",
      "4259  trump          prize        2        1  0.666667  0.333333  0.333333   \n",
      "4260  trump       sustains        2        1  0.666667  0.333333  0.333333   \n",
      "4262  trump           deep        2        1  0.666667  0.333333  0.333333   \n",
      "4266  trump       treasure        2        1  0.666667  0.333333  0.333333   \n",
      "4269  trump        chamber        2        1  0.666667  0.333333  0.333333   \n",
      "4270  trump      listening        2        1  0.666667  0.333333  0.333333   \n",
      "4272  trump        patriot        2        1  0.666667  0.333333  0.333333   \n",
      "4274  trump        intense        2        1  0.666667  0.333333  0.333333   \n",
      "4276  trump       homeland        2        1  0.666667  0.333333  0.333333   \n",
      "4279  trump          souls        2        1  0.666667  0.333333  0.333333   \n",
      "4284  trump     scientific        2        1  0.666667  0.333333  0.333333   \n",
      "4285  trump  breakthroughs        2        1  0.666667  0.333333  0.333333   \n",
      "4287  trump            art        2        1  0.666667  0.333333  0.333333   \n",
      "4289  trump          erase        2        1  0.666667  0.333333  0.333333   \n",
      "4290  trump           draw        2        1  0.666667  0.333333  0.333333   \n",
      "4291  trump        ancient        2        1  0.666667  0.333333  0.333333   \n",
      "4292  trump         wisdom        2        1  0.666667  0.333333  0.333333   \n",
      "4294  trump        regions        2        1  0.666667  0.333333  0.333333   \n",
      "4295  trump        unleash        2        1  0.666667  0.333333  0.333333   \n",
      "4296  trump    foundations        2        1  0.666667  0.333333  0.333333   \n",
      "4298  trump        vehicle        2        1  0.666667  0.333333  0.333333   \n",
      "4299  trump       survived        2        1  0.666667  0.333333  0.333333   \n",
      "4301  trump      prospered        2        1  0.666667  0.333333  0.333333   \n",
      "4302  trump      cherished        2        1  0.666667  0.333333  0.333333   \n",
      "4303  trump      unfolding        2        1  0.666667  0.333333  0.333333   \n",
      "4304  trump    peacemaking        2        1  0.666667  0.333333  0.333333   \n",
      "4307  trump        resolve        2        1  0.666667  0.333333  0.333333   \n",
      "4308  trump    flourishing        2        1  0.666667  0.333333  0.333333   \n",
      "4311  trump       thankful        2        1  0.666667  0.333333  0.333333   \n",
      "4314  trump          glory        2        1  0.666667  0.333333  0.333333   \n",
      "\n",
      "            fn  \n",
      "9     0.645866  \n",
      "35    0.659111  \n",
      "30    0.632822  \n",
      "20    0.671897  \n",
      "2353  0.354134  \n",
      "101   0.776515  \n",
      "2368  0.340889  \n",
      "108   0.683495  \n",
      "87    0.593985  \n",
      "2344  0.367178  \n",
      "83    0.607143  \n",
      "23    0.647059  \n",
      "3     0.597990  \n",
      "2360  0.328103  \n",
      "14    0.663717  \n",
      "2351  0.406015  \n",
      "64    0.646259  \n",
      "41    0.724806  \n",
      "2346  0.392857  \n",
      "16    0.521739  \n",
      "2356  0.316505  \n",
      "2341  0.402010  \n",
      "2386  0.478261  \n",
      "259   0.877907  \n",
      "96    0.540856  \n",
      "2366  0.352941  \n",
      "102   0.632558  \n",
      "86    0.697436  \n",
      "17    0.560000  \n",
      "183   0.689655  \n",
      "...        ...  \n",
      "4259  0.666667  \n",
      "4260  0.666667  \n",
      "4262  0.666667  \n",
      "4266  0.666667  \n",
      "4269  0.666667  \n",
      "4270  0.666667  \n",
      "4272  0.666667  \n",
      "4274  0.666667  \n",
      "4276  0.666667  \n",
      "4279  0.666667  \n",
      "4284  0.666667  \n",
      "4285  0.666667  \n",
      "4287  0.666667  \n",
      "4289  0.666667  \n",
      "4290  0.666667  \n",
      "4291  0.666667  \n",
      "4292  0.666667  \n",
      "4294  0.666667  \n",
      "4295  0.666667  \n",
      "4296  0.666667  \n",
      "4298  0.666667  \n",
      "4299  0.666667  \n",
      "4301  0.666667  \n",
      "4302  0.666667  \n",
      "4303  0.666667  \n",
      "4304  0.666667  \n",
      "4307  0.666667  \n",
      "4308  0.666667  \n",
      "4311  0.666667  \n",
      "4314  0.666667  \n",
      "\n",
      "[4315 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "metrics_unfiltered = add_statistics(df)\n",
    "print(metrics_unfiltered.sort_values(by=['c_author', 'c_others'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the priors\n",
    "\n",
    "We have two sources, therefore, each source should have a prior of 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obama: 0.50\n",
      "trump: 0.50\n"
     ]
    }
   ],
   "source": [
    "prior = {}\n",
    "users = sorted(metrics_unfiltered['author'].unique())\n",
    "for u in users:\n",
    "    prior[u] = 1 / len(metrics_unfiltered['author'].unique())\n",
    "    print(\"{}: {:.2f}\".format(u, prior[u]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing text snipets\n",
    "\n",
    "The analyse_texts() function converts the metrics dataframe in a format that the update_probability() function can handle. \n",
    "\n",
    "The function will go through each word of the text snipet, check if there is an associated probability to it (only probabilities of the target author are received in parameter), and if a word isn't found, which means that that author never used that word before, we assign a probability 50% to it, basically not altering the posterior probability.\n",
    "\n",
    "Experimentation notes: I did perform some test by setting a 49% probability instead of 50% for never used words, which added a negative bias for new words, but I removed it because I also added a filtering on the words to keep only the bottom 80%, removing over repeated English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_texts(prior, texts, probabilities, debug):\n",
    "    posterior = prior\n",
    "    tests = {}\n",
    "\n",
    "    for tt in texts:\n",
    "        prob = probabilities.loc[probabilities['word'] == tt]\n",
    "\n",
    "        if tt not in tests.keys():\n",
    "            if not prob.empty:\n",
    "                tests[tt] = {\n",
    "                    'True': {\n",
    "                        'Positive': prob.head(1)['tp'].values[0],\n",
    "                        'Negative': prob.head(1)['tn'].values[0]\n",
    "                    },\n",
    "                    'False': {\n",
    "                        'Positive': prob.head(1)['fp'].values[0],\n",
    "                        'Negative': prob.head(1)['fn'].values[0]\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                tests[tt] = {\n",
    "                    'True': {\n",
    "                        'Positive': 0.5,\n",
    "                        'Negative': 0.5\n",
    "                    },\n",
    "                    'False': {\n",
    "                        'Positive': 0.5,\n",
    "                        'Negative': 0.5\n",
    "                    }\n",
    "                }\n",
    "        if not prob.empty:\n",
    "            posterior = update_probability(posterior, tt, tests, 'True', debug)\n",
    "        else:\n",
    "            posterior = update_probability(posterior, tt, tests, 'False', debug)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_text(lines, prior, metrics):\n",
    "    for ll in lines:\n",
    "        print('Given the text \"{}\"...'.format(\" \".join(ll)))\n",
    "        for author in prior.keys():\n",
    "            posterior = analyse_texts(prior[author], ll, metrics.loc[metrics['author'] == author], False)\n",
    "            print(\"\\tProbability that {} wrote this text is: {:.3f}%\".format(author, 100 * posterior))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering metrics to keep only the buttom 80% used words.\n",
    "\n",
    "I am removing over repeating words here because they are skewing the probablities in the direction of the author that uses the most articles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     author        word c_author c_others        tp        tn        fp  \\\n",
      "3705  trump      change        5       68  0.068493  0.931507  0.931507   \n",
      "2821  trump      cannot        5       31  0.138889  0.861111  0.861111   \n",
      "1468  obama       doing        5       26  0.161290  0.838710  0.838710   \n",
      "3266  trump  washington        5       26  0.161290  0.838710  0.838710   \n",
      "2342  trump          am        5       25  0.166667  0.833333  0.833333   \n",
      "\n",
      "            fn  \n",
      "3705  0.068493  \n",
      "2821  0.138889  \n",
      "1468  0.161290  \n",
      "3266  0.161290  \n",
      "2342  0.166667  \n",
      "2955\n"
     ]
    }
   ],
   "source": [
    "metrics = metrics_unfiltered[metrics_unfiltered.c_author < metrics_unfiltered.c_author.quantile(.8)]\n",
    "print(metrics.sort_values(by=['c_author', 'c_others'], ascending=False).head())\n",
    "print(len(metrics['word'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying our model using tweets\n",
    "\n",
    "We are now ready to throw tweets at our model. The text comes from the following tweets:\n",
    "\n",
    "* tweet 1: https://twitter.com/BarackObama/status/1044690296917962754\n",
    "* tweet 2: https://twitter.com/realDonaldTrump/status/1045444544068812800\n",
    "* tweet 3: https://twitter.com/realDonaldTrump/status/1045003711104331776\n",
    "* tweet 4: https://twitter.com/BarackObama/status/1039512025406349312\n",
    "* tweet 5: https://twitter.com/BarackObama/status/1034892109868945409\n",
    "* tweet 6: https://twitter.com/realDonaldTrump/status/1043966388182953984\n",
    "\n",
    "This function could also be used to analyse each sentenses from an anonymous op-ed against the known speeches of candidate writers, and evaluate if a full text is likely or not to have been written by multiple authors or from a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the text \"the antidote to government by a powerful few is government by the organized energized many this national voter registration day make sure you're registered vote early if you can or show up on november 6 this moment is too important to sit out\"...\n",
      "\tProbability that obama wrote this text is: 10.917%\n",
      "\tProbability that trump wrote this text is: 11.868%\n",
      "\n",
      "\n",
      "Given the text \"judge kavanaugh showed america exactly why i nominated him his testimony was powerful honest and riveting democrats search and destroy strategy is disgraceful and this process has been a total sham and effort to delay obstruct and resist the senate must vote\"...\n",
      "\tProbability that obama wrote this text is: 62.327%\n",
      "\tProbability that trump wrote this text is: 90.909%\n",
      "\n",
      "\n",
      "Given the text \"congressman lee zeldin is doing a fantastic job in d c tough and smart he loves our country and will always be there to do the right thing he has my complete and total endorsement\"...\n",
      "\tProbability that obama wrote this text is: 23.529%\n",
      "\tProbability that trump wrote this text is: 96.774%\n",
      "\n",
      "\n",
      "Given the text \"we will always remember everyone we lost on 9 11 thank the first responders who keep us safe and honor all who defend our country and the ideals that bind us together there's nothing our resilience and resolve can t overcome and no act of terror can ever change who we are\"...\n",
      "\tProbability that obama wrote this text is: 98.361%\n",
      "\tProbability that trump wrote this text is: 0.711%\n",
      "\n",
      "\n",
      "Given the text \"yesterday i met with high school students on chicago s southwest side who spent the summer learning to code some pretty cool apps michelle and i are proud to support programs that invest in local youth and we re proud of these young people\"...\n",
      "\tProbability that obama wrote this text is: 99.174%\n",
      "\tProbability that trump wrote this text is: 1.550%\n",
      "\n",
      "\n",
      "Given the text \"going to new york will be with prime minister abe of japan tonight talking military and trade we have done much to help japan would like to see more of a reciprocal relationship it will all work out\"...\n",
      "\tProbability that obama wrote this text is: 16.667%\n",
      "\tProbability that trump wrote this text is: 85.039%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines_of_text = [\"The antidote to government by a powerful few is government by the organized, energized many. This National Voter Registration Day, make sure you're registered, vote early if you can, or show up on November 6. This moment is too important to sit out.\",\n",
    "                \"Judge Kavanaugh showed America exactly why I nominated him. His testimony was powerful, honest, and riveting. Democrats’ search and destroy strategy is disgraceful and this process has been a total sham and effort to delay, obstruct, and resist. The Senate must vote!\",\n",
    "                \"Congressman Lee Zeldin is doing a fantastic job in D.C. Tough and smart, he loves our Country and will always be there to do the right thing. He has my Complete and Total Endorsement!\",\n",
    "                \"We will always remember everyone we lost on 9/11, thank the first responders who keep us safe, and honor all who defend our country and the ideals that bind us together. There's nothing our resilience and resolve can’t overcome, and no act of terror can ever change who we are.\",\n",
    "                \"Yesterday I met with high school students on Chicago’s Southwest side who spent the summer learning to code some pretty cool apps. Michelle and I are proud to support programs that invest in local youth and we’re proud of these young people.\",\n",
    "                \"Going to New York. Will be with Prime Minister Abe of Japan tonight, talking Military and Trade. We have done much to help Japan, would like to see more of a reciprocal relationship. It will all work out!\"]\n",
    "\n",
    "clean_lines_of_texts = clean_lines(lines_of_text)\n",
    "\n",
    "posterior = lookup_text(clean_lines_of_texts, prior, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve?\n",
    "\n",
    "This notebook uses Naive Bayes, which naively consider each word as independent events. This isn't true in reality, but allows us to run a simple function over all words without too much computer horse power. \n",
    "\n",
    "* The accuracy could most probably be improved by using markov chains. To do in a future notebook...\n",
    "* An other approach would be through machine learning, whatever the model used... also for a future notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
